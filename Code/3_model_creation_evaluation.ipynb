{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'world/life'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'world/life'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking if features are numeric:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m relevant_features:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_numeric_dtype(df[col]):\n\u001b[1;32m     43\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not numeric.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'world/life'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import silhouette_score\n",
    "import logging\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --------------------- Setup --------------------- #\n",
    "np.random.seed(42)\n",
    "sns.set(style=\"whitegrid\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# --------------------- Load Data --------------------- #\n",
    "DATA_PATH = \"../data/2_cleaned_songs_dataset.csv\"\n",
    "IMAGE_DIR = \"../images\"\n",
    "MODEL_DIR = \"../models\"\n",
    "OUTPUT_PATH = \"../data/3_clustered_songs_dataset.csv\"\n",
    "\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "logging.info(f\"Data loaded with shape: {df.shape}\")\n",
    "\n",
    "# --------------------- Select & Preprocess Features --------------------- #\n",
    "# List of features to include, excluding 'len' and other non-useful features\n",
    "relevant_features = [\n",
    "    'dating', 'violence', 'world/life', 'night/time', 'shake the audience', \n",
    "    'family/gospel', 'romantic', 'communication', 'obscene', 'music', \n",
    "    'movement/places', 'light/visual perceptions', 'family/spiritual', \n",
    "    'sadness', 'feelings', 'topic'\n",
    "]\n",
    "\n",
    "# Check if features are numeric\n",
    "logging.info(\"Checking if features are numeric:\")\n",
    "for col in relevant_features:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "        logging.info(f\"Column '{col}' is not numeric.\")\n",
    "    else:\n",
    "        logging.info(f\"Column '{col}' is numeric.\")\n",
    "\n",
    "# Convert non-numeric columns to numeric if necessary (for example, encoding categorical data)\n",
    "# If you have any categorical variables, you should encode them. Here we use label encoding as an example:\n",
    "\n",
    "for col in relevant_features:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        logging.info(f\"Column '{col}' has been encoded.\")\n",
    "\n",
    "# Filter out only relevant features from the dataframe\n",
    "features = df[relevant_features]\n",
    "\n",
    "# Handle missing values by filling with column means\n",
    "features.fillna(features.mean(), inplace=True)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler().fit(features)\n",
    "scaled_features = scaler.transform(features)\n",
    "\n",
    "# --------------------- Dimensionality Reduction --------------------- #\n",
    "# PCA can be used to reduce dimensionality for better performance and visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(scaled_features)\n",
    "logging.info(f\"PCA applied: Explained variance = {np.sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "# --------------------- Silhouette Score Evaluation for KMeans --------------------- #\n",
    "k_values = range(2, 11)\n",
    "silhouette_scores = []\n",
    "\n",
    "logging.info(\"Evaluating silhouette scores for KMeans...\")\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(reduced_features)\n",
    "    score = silhouette_score(reduced_features, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "    logging.info(f\"Silhouette Score for k={k}: {score:.4f}\")\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, silhouette_scores, marker='o', color='orange')\n",
    "plt.title(\"Silhouette Scores for KMeans Clustering\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMAGE_DIR, \"3.1_silhouette_scores.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Optimal number of clusters based on silhouette score\n",
    "optimal_k = k_values[np.argmax(silhouette_scores)]\n",
    "logging.info(f\"Optimal number of clusters (KMeans) based on silhouette score: {optimal_k}\")\n",
    "\n",
    "# --------------------- Advanced Clustering Techniques --------------------- #\n",
    "# 1. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "df[\"dbscan_cluster\"] = dbscan.fit_predict(reduced_features)\n",
    "logging.info(\"DBSCAN clustering completed.\")\n",
    "logging.info(f\"Number of clusters found by DBSCAN: {len(np.unique(df['dbscan_cluster'])) - (1 if -1 in df['dbscan_cluster'].values else 0)}\")\n",
    "\n",
    "# 2. Agglomerative Clustering\n",
    "agg_clust = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "df[\"agg_cluster\"] = agg_clust.fit_predict(reduced_features)\n",
    "logging.info(\"Agglomerative Clustering completed.\")\n",
    "\n",
    "# --------------------- Apply Optimal KMeans --------------------- #\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "df[\"cluster\"] = kmeans.fit_predict(reduced_features)\n",
    "\n",
    "# Save the model and scaler\n",
    "joblib.dump(scaler, os.path.join(MODEL_DIR, \"scaler.pkl\"))\n",
    "joblib.dump(kmeans, os.path.join(MODEL_DIR, \"kmeans_model.pkl\"))\n",
    "joblib.dump(dbscan, os.path.join(MODEL_DIR, \"dbscan_model.pkl\"))\n",
    "joblib.dump(agg_clust, os.path.join(MODEL_DIR, \"agg_clust_model.pkl\"))\n",
    "\n",
    "# Save clustered data\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "logging.info(f\"Clustered data saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "# --------------------- Visualize Clusters --------------------- #\n",
    "# Visualize in 2D using PCA-reduced features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=df[\"cluster\"], palette=\"viridis\", alpha=0.7)\n",
    "plt.title(\"Clusters Based on PCA-Reduced Features (KMeans)\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.savefig(os.path.join(IMAGE_DIR, \"3.2_song_clusters_kmeans.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Visualize DBSCAN Clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=df[\"dbscan_cluster\"], palette=\"coolwarm\", alpha=0.7)\n",
    "plt.title(\"DBSCAN Clusters\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.savefig(os.path.join(IMAGE_DIR, \"3.3_song_clusters_dbscan.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Visualize Agglomerative Clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=df[\"agg_cluster\"], palette=\"Set2\", alpha=0.7)\n",
    "plt.title(\"Agglomerative Clusters\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.savefig(os.path.join(IMAGE_DIR, \"3.4_song_clusters_agg.png\"))\n",
    "plt.show()\n",
    "\n",
    "# --------------------- Cluster Summaries --------------------- #\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df[df[\"cluster\"] == cluster]\n",
    "    logging.info(f\"\\nSummary for Cluster {cluster}:\")\n",
    "    logging.info(cluster_data.describe())\n",
    "\n",
    "# Additional DBSCAN and Agglomerative cluster summaries\n",
    "logging.info(f\"\\nSummary for DBSCAN clusters:\")\n",
    "logging.info(df.groupby(\"dbscan_cluster\").describe())\n",
    "\n",
    "logging.info(f\"\\nSummary for Agglomerative clusters:\")\n",
    "logging.info(df.groupby(\"agg_cluster\").describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start off by importing our libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import silhouette_score\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been loaded with shape: (28362, 22)\n"
     ]
    }
   ],
   "source": [
    "# We will set up our paths and load our data\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "DATA_PATH = \"../data/2_cleaned_songs_dataset.csv\"\n",
    "IMAGE_DIR = \"../images\"\n",
    "MODEL_DIR = \"../models\"\n",
    "OUTPUT_PATH = \"../data/3_clustered_songs_dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"The data has been loaded with shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the columns of the DataFrame to check what is available\n",
    "logging.info(\"Available columns in the DataFrame:\")\n",
    "logging.info(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'world/life'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'world/life'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking if features are numeric:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m relevant_features:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_numeric_dtype(df[col]):\n\u001b[1;32m     13\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not numeric.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'world/life'"
     ]
    }
   ],
   "source": [
    "# We will select and preprocess the features\n",
    "relevant_features = [\n",
    "    'dating', 'violence', 'world/life', 'night/time', 'shake the audience', \n",
    "    'family/gospel', 'romantic', 'communication', 'obscene', 'music', \n",
    "    'movement/places', 'light/visual perceptions', 'family/spiritual', \n",
    "    'sadness', 'feelings', 'topic'\n",
    "]\n",
    "\n",
    "# We will check if features are numeric\n",
    "logging.info(\"Checking if features are numeric:\")\n",
    "for col in relevant_features:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "        logging.info(f\"Column '{col}' is not numeric.\")\n",
    "    else:\n",
    "        logging.info(f\"Column '{col}' is numeric.\")\n",
    "\n",
    "# Convert non-numeric columns to numeric if necessary \n",
    "for col in relevant_features:\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        logging.info(f\"Column '{col}' has been encoded.\")\n",
    "\n",
    "# Filter out only relevant features from the dataframe\n",
    "features = df[relevant_features]\n",
    "\n",
    "# Handle missing values by filling with column means\n",
    "features.fillna(features.mean(), inplace=True)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler().fit(features)\n",
    "scaled_features = scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(scaled_features)\n",
    "print(f\"The PCA has been applied: Explained variance = {np.sum(pca.explained_variance_ratio_):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will obtain the Silhouette Score for KMeans\n",
    "k_values = range(2, 11)\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(reduced_features)\n",
    "    score = silhouette_score(reduced_features, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"Silhouette Score for k={k}: {score:.4f}\")\n",
    "\n",
    "# We will plot the silhouette scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, silhouette_scores, marker='o', color='orange')\n",
    "plt.title(\"Silhouette Scores for KMeans Clustering\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMAGE_DIR, \"3.1_silhouette_scores.png\"))\n",
    "plt.show()\n",
    "\n",
    "# We will find the optimal number of clusters based on the silhouette score\n",
    "optimal_k = k_values[np.argmax(silhouette_scores)]\n",
    "print(f\"The optimal number of clusters (KMeans) based on silhouette score: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform advanced Clustering Techniques: DBSCAN & Agglomerative Clustering \n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "df[\"dbscan_cluster\"] = dbscan.fit_predict(reduced_features)\n",
    "print(f\"DBSCAN clustering completed. Number of clusters found: {len(np.unique(df['dbscan_cluster'])) - (1 if -1 in df['dbscan_cluster'].values else 0)}\")\n",
    "\n",
    "agg_clust = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "df[\"agg_cluster\"] = agg_clust.fit_predict(reduced_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will apply Optimal KMeans, save the model, scaler and clustered data\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "df[\"cluster\"] = kmeans.fit_predict(reduced_features)\n",
    "\n",
    "joblib.dump(scaler, os.path.join(MODEL_DIR, \"scaler.pkl\"))\n",
    "joblib.dump(kmeans, os.path.join(MODEL_DIR, \"kmeans_model.pkl\"))\n",
    "joblib.dump(dbscan, os.path.join(MODEL_DIR, \"dbscan_model.pkl\"))\n",
    "joblib.dump(agg_clust, os.path.join(MODEL_DIR, \"agg_clust_model.pkl\"))\n",
    "\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Clustered data saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will visualize the clusters: KMeans, DBSCAN and Agglomerative Clusters\n",
    "# KMeans\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=df[\"cluster\"], palette=\"viridis\", alpha=0.7)\n",
    "plt.title(\"Clusters Based on PCA-Reduced Features (KMeans)\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.savefig(os.path.join(IMAGE_DIR, \"3.2_song_clusters_kmeans.png\"))\n",
    "plt.show()\n",
    "\n",
    "# DBSCAN\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=df[\"dbscan_cluster\"], palette=\"coolwarm\", alpha=0.7)\n",
    "plt.title(\"DBSCAN Clusters\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.savefig(os.path.join(IMAGE_DIR, \"3.3_song_clusters_dbscan.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Agglomerative\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=df[\"agg_cluster\"], palette=\"Set2\", alpha=0.7)\n",
    "plt.title(\"Agglomerative Clusters\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.savefig(os.path.join(IMAGE_DIR, \"3.4_song_clusters_agg.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will print summaries of all 3 clustering methods\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df[df[\"cluster\"] == cluster]\n",
    "    print(f\"\\nSummary for Cluster {cluster}:\")\n",
    "    print(cluster_data.describe())\n",
    "\n",
    "print(\"\\nSummary for DBSCAN clusters:\")\n",
    "print(df.groupby(\"dbscan_cluster\").describe())\n",
    "\n",
    "print(\"\\nSummary for Agglomerative clusters:\")\n",
    "print(df.groupby(\"agg_cluster\").describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
